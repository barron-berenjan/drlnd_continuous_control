{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Algorithm\n",
        "\n",
        "The learning algorithm implemented for this assignment is <a href=\"https://arxiv.org/pdf/1802.09477.pdf\">Twin Delayed DDPG (TD3)</a> that builds on <a href=\"https://arxiv.org/abs/1509.02971\">DDPG</a> - an off-policy, actor/critic network that inherits concepts from Deep-Q Learning such as _Experience Replay_ and _Fixed Q-Targets_ to train environments with continuous action spaces. TD3 aims to address the problem of overestimation bias and the accumulation of error in temporal difference methods in which an estimate of the value function is derived from another estimate, creating a cascading effect of imprecise estimates. Due to overestimation bias, this accumulated error can cause arbitrarily bad states to be estimated as high value, resulting in suboptimal policy updates and divergent behavior. To address this, TD3 introduce **two critic networks** to estimate Q-values, using the smaller of the two to form the targets in the Bellman error loss functions. Some other differences with DDPG include:\n",
        "\n",
        " - the use of noise to regularize target actions\n",
        " - delayed update to the policy network\n",
        "  \n",
        "For this assignment, I trained an agent using both __DDPG__ and __Twin Delayed DDPG (TD3)__. The learning plots are displayed below. The DDPG agent took ~3500 episodes to complete the task (i.e. achieve an average score of +30 over 100 consecutive episodes) and i found it difficult to settle on good hyper-parameters. Also, as can be observed, there is high variance across episodes and the trajectory is not as smooth as training the agent with TD3 which took ~442 episodes to complete the same task! \n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "DDPG            |  Twin Delayed DDPG (TD3)\n",
        ":-------------------------:|:-------------------------:\n",
        "<img src=\"img/DPDG_Continuous_Control.png\"> |  <img src=\"img/TD3_Continuous_Control.png\">\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "## Architecture and Hyperparameters\n",
        "\n",
        "\n",
        "The network architecture and hyperparameters for the Twin Delayed DDPG are below:\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<center> Actor </center>\n",
        "\n",
        "| Layer | Input  | Output   |   \n",
        "|:-------|:--------|:----------|\n",
        "|FC1    |   33 (state space)  |  400       |   \n",
        "|FC2    |   400   |  300      |   \n",
        "|FC3    |   300     |   4 (action space)    |\n",
        "    \n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<center> Critic </center>\n",
        "\n",
        "| Layer | Input  | Output   |   \n",
        "|:-------|:--------|:----------|\n",
        "|FC1    |   33 (state space) + 4 (action space)   |  400       |   \n",
        "|FC2    |   400    |  300      |   \n",
        "|FC3    |   300     |   1  (Q-value)  |\n",
        "\n",
        "<br>\n",
        "\n",
        "###  Parameters used for training :\n",
        "\n",
        "```python\n",
        "\n",
        "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
        "BATCH_SIZE = 512        # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "TAU = 5e-3              # for soft update of target parameters\n",
        "LR_ACTOR = 1e-5         # learning rate of the actor\n",
        "LR_CRITIC = 1e-4        # learning rate of the critic\n",
        "WEIGHT_DECAY = 0        # L2 weight decay\n",
        "UPDATE_EVERY = 2        # Steps to update agent\n",
        "\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Ideas for Future Work\n",
        "\n",
        "A few things that can be tried to improve model performance are:\n",
        "\n",
        "- Tuning of hyper-parameters:\n",
        "\n",
        "\n",
        "    - number of hidden cells\n",
        "    - number of hidden layers\n",
        "    - actor/ critic learning rates\n",
        "    - batch size\n",
        "    - noise regularization\n",
        "    \n",
        "- Train the agent using D4PG, A2C and PPO and see how they compare."
      ],
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "nteract": {
      "version": "0.23.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}